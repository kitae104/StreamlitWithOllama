import streamlit as st
import os, hashlib, io

from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS

# âœ… ìµœì‹  ê¶Œì¥ ì„í¬íŠ¸ ê²½ë¡œ
from langchain_ollama.llms import OllamaLLM
from langchain_ollama.embeddings import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

st.set_page_config(page_title="PDF/TXT Q&A ì±—ë´‡")
st.title("ğŸ“„ PDF/TXT íŒŒì¼ Q&A ì±—ë´‡")
st.markdown("Ollamaì™€ Streamlitì„ ì´ìš©í•œ RAG ì±—ë´‡")

# ---------------------------
# Ollama ì„¤ì •
# ---------------------------
OLLAMA_URL = "http://127.0.0.1:11434"
LLM_MODEL = "exaone3.5:2.4b" # ë˜ëŠ” "gemma:2b"
EMB_MODEL = "bge-m3" # ë˜ëŠ” "nomic-embed-text"

llm = OllamaLLM(
    model=LLM_MODEL,
    base_url=OLLAMA_URL,
    streaming=True,
    # â†“ langchain_ollamaëŠ” ì•„ë˜ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
    model_kwargs={
        "num_predict": 256,     # ìµœëŒ€ ìƒì„± í† í° (í•„ìˆ˜)
        "num_ctx": 2048,        # ë¬¸ì„œ+í”„ë¡¬í”„íŠ¸ ì´ ê¸¸ì´ ìƒí•œ(í•„ìš” ì´ìƒ í‚¤ìš°ì§€ ì•Šê¸°)
        "temperature": 0.2,     # ë‚®ì„ìˆ˜ë¡ ì¼ê´€/ì§§ì€ ë‹µë³€
        "top_p": 0.9,
        "repeat_penalty": 1.1,
        "keep_alive": "30m",    # í”„ë¡œì„¸ìŠ¤ ìœ ì§€(ì½œë“œìŠ¤íƒ€íŠ¸ ë°©ì§€)
        "num_thread": 0,      # 0=ìë™(ì½”ì–´ìˆ˜). CPUë¼ë©´ ëª…ì‹œì  ì„¤ì •ë„ ê³ ë ¤
    }
)
embeddings = OllamaEmbeddings(model=EMB_MODEL, base_url=OLLAMA_URL)

# ---------------------------
# ìœ í‹¸: íŒŒì¼ í•´ì‹œ + íŒŒë¼ë¯¸í„°ë¡œ ìºì‹œ í‚¤ ìƒì„±
# ---------------------------
def file_digest(file_bytes: bytes, extra: str) -> str:
    h = hashlib.sha256()
    h.update(file_bytes)
    h.update(extra.encode("utf-8"))
    return h.hexdigest()

# ---------------------------
# ìºì‹œ: ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ/ìƒì„±
# ---------------------------
@st.cache_resource(show_spinner=False)
def build_or_load_faiss(index_dir: str, docs, chunk_size: int, chunk_overlap: int):
    # docs -> splits -> FAISS
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    splits = splitter.split_documents(docs)
    db = FAISS.from_documents(splits, embeddings)
    os.makedirs(index_dir, exist_ok=True)
    db.save_local(index_dir)
    return db

@st.cache_resource(show_spinner=False)
def load_faiss(index_dir: str):
    return FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)

# ---------------------------
# íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
# ---------------------------
def load_docs_from_bytes(name: str, raw: bytes):
    ext = os.path.splitext(name)[1].lower()
    temp_dir = "./temp"
    os.makedirs(temp_dir, exist_ok=True)
    file_path = os.path.join(temp_dir, name)
    with open(file_path, "wb") as f:
        f.write(raw)

    if ext == ".pdf":
        loader = PyPDFLoader(file_path)
    elif ext == ".txt":
        loader = TextLoader(file_path, encoding="utf-8")
    else:
        st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (PDF, TXTë§Œ ì§€ì›)")
        return None
    return loader.load()

# ---------------------------
# ì‚¬ì´ë“œë°”: ì—…ë¡œë“œ & ì¸ë±ìŠ¤ ì¤€ë¹„
# ---------------------------
with st.sidebar:
    st.header("íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader("PDF ë˜ëŠ” TXT íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf", "txt"])

    # ì¸ë±ìŠ¤ íŒŒë¼ë¯¸í„°(ë³€ê²½ ì‹œ ìƒˆ ìºì‹œ í‚¤ê°€ ìƒì„±ë˜ë„ë¡)
    chunk_size = st.number_input("chunk_size", 300, 4000, 500, 100)
    chunk_overlap = st.number_input("chunk_overlap", 0, 800, 100, 50)

    if uploaded_file:
        raw = uploaded_file.read()  # ì¤‘ìš”: ë°”ì´íŠ¸ ì½ê³  ì•„ë˜ì—ì„œ ì¬ì‚¬ìš©
        st.success("íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ!")

        # íŒŒì¼+ì„¤ì •+ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±
        extra = f"{uploaded_file.name}|{chunk_size}|{chunk_overlap}|{LLM_MODEL}|{EMB_MODEL}"
        digest = file_digest(raw, extra)
        index_dir = f"./indexes/{digest}"

        # ì„¸ì…˜ì— í˜„ì¬ ì„ íƒ íŒŒì¼ì˜ digest ì €ì¥
        st.session_state["current_digest"] = digest

        # ì´ë¯¸ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±
        if os.path.isdir(index_dir) and os.path.exists(os.path.join(index_dir, "index.faiss")):
            with st.spinner("ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘..."):
                db = load_faiss(index_dir)
        else:
            with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„°í™” ì¤‘... (ìµœì´ˆ 1íšŒ)"):
                docs = load_docs_from_bytes(uploaded_file.name, raw)
                if docs:
                    db = build_or_load_faiss(index_dir, docs, chunk_size, chunk_overlap)
                else:
                    db = None

        if db:
            st.session_state["db"] = db
            st.success("ì¤€ë¹„ ì™„ë£Œ! (ìºì‹œ/ì¸ë±ìŠ¤ ì‚¬ìš©)")

# ---------------------------
# ì±„íŒ… ìƒíƒœ
# ---------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# ---------------------------
# ì§ˆì˜ (LCEL ë°©ì‹ìœ¼ë¡œ ì¬êµ¬ì„±)
# ---------------------------
prompt = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ìƒê°í•˜ëŠ” ì¤‘..."):
            db = st.session_state.get("db")
            if db is None:
                st.warning("ë¨¼ì € ì™¼ìª½ì—ì„œ PDF/TXTë¥¼ ì—…ë¡œë“œí•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ ì¤€ë¹„í•´ ì£¼ì„¸ìš”.")
                st.stop()

            # 1. ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
            # ğŸ’¡ ì†ë„ë¥¼ ìœ„í•´ k=2 ì •ë„ë¡œ ë‚®ê²Œ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
            retriever = db.as_retriever(search_kwargs={"k": 2})

            # 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
            template = """
            ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ ì£¼ì„¸ìš”.
            ë¬¸ì„œì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

            ë¬¸ì„œ:
            {context}

            ì§ˆë¬¸:
            {question}
            """
            prompt_template = ChatPromptTemplate.from_template(template)

            # 3. ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜ ì •ì˜
            def format_docs(docs):
                return "\n\n".join(doc.page_content for doc in docs)

            # 4. LCEL ì²´ì¸ êµ¬ì„±
            chain = (
                {"context": retriever | format_docs, "question": RunnablePassthrough()}
                | prompt_template
                | llm
                | StrOutputParser()
            )

            # 5. ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ ë° ì¶œë ¥
            # st.write_streamì€ ì œë„ˆë ˆì´í„°ë¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.
            # chain.stream()ì€ ê° í† í°ì„ ìƒì„±í•˜ëŠ” ì œë„ˆë ˆì´í„°ì…ë‹ˆë‹¤.
            answer = st.write_stream(chain.stream(prompt))

            # --- ì°¸ê³  ë¬¸ì„œ í‘œì‹œëŠ” ìŠ¤íŠ¸ë¦¼ì´ ëë‚œ í›„ ì²˜ë¦¬ ---
            srcs = retriever.get_relevant_documents(prompt)
            if srcs:
                names = list({os.path.basename(s.metadata.get("source", "")) for s in srcs})
                if names:
                    ref_text = "\n\n---\n**ì°¸ê³  ë¬¸ì„œ**: " + ", ".join(names)
                    st.markdown(ref_text)
                    full_answer = answer + ref_text
                else:
                    full_answer = answer
                st.session_state.messages.append({"role": "assistant", "content": full_answer})
            else:
                st.session_state.messages.append({"role": "assistant", "content": answer})
